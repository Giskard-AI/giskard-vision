{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from face_alignment import FaceAlignment, LandmarksType\n",
    "\n",
    "from giskard_vision.landmark_detection.dataloaders.loaders import DataLoaderFFHQ, DataLoader300W\n",
    "from giskard_vision.landmark_detection.dataloaders.wrappers import (\n",
    "    CroppedDataLoader,\n",
    "    ResizedDataLoader,\n",
    "    ColoredDataLoader,\n",
    "    BlurredDataLoader,\n",
    "    FilteredDataLoader,\n",
    "    HeadPoseDataLoader,\n",
    "    EthnicityDataLoader,\n",
    "    CachedDataLoader,\n",
    ")\n",
    "\n",
    "from giskard_vision.landmark_detection.models.wrappers import OpenCVWrapper, FaceAlignmentWrapper\n",
    "from giskard_vision.landmark_detection.tests.performance import NMEMean\n",
    "from giskard_vision.landmark_detection.tests.base import Test, TestDiff\n",
    "from giskard_vision.landmark_detection.marks.facial_parts import FacialParts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading FFHQ dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl_ref = DataLoaderFFHQ(\"ffhq\")\n",
    "#dl_ref = DataLoader300W(dir_path=\"_300W_full/01_Indoor\")\n",
    "dl_ref = DataLoader300W(dir_path=\"300W/sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading landmark-detection models\n",
    "- FaceAlignment\n",
    "- OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from : lbfmodel.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<giskard_vision.landmark_detection.models.wrappers.FaceAlignmentWrapper at 0x1226e1750>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = {\n",
    "    \"FaceAlignment\": FaceAlignmentWrapper(model=FaceAlignment(LandmarksType.TWO_D, device=\"cpu\", flip_input=False)),\n",
    "    \"OpenCV\": OpenCVWrapper(),\n",
    "}\n",
    "models.pop(\"FaceAlignment\")  # takes a long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1: Cropped Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCVWrapper: Face not detected in processed image of batch 1 and index 0.\n",
      "OpenCVWrapper: Face not detected in processed image of batch 5 and index 0.\n",
      "/Users/rak/Documents/giskard-vision/giskard_vision/landmark_detection/tests/performance.py:39: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(nes, axis=1)\n",
      "OpenCVWrapper: Face not detected in processed image of batch 1 and index 0.\n",
      "OpenCVWrapper: Face not detected in processed image of batch 2 and index 0.\n",
      "OpenCVWrapper: Face not detected in processed image of batch 3 and index 0.\n",
      "OpenCVWrapper: Face not detected in processed image of batch 4 and index 0.\n"
     ]
    }
   ],
   "source": [
    "facial_parts = [FacialParts.LEFT_HALF.value, FacialParts.RIGHT_HALF.value]\n",
    "\n",
    "for model in models.values():\n",
    "    for fp in facial_parts:\n",
    "        dl = CroppedDataLoader(dl_ref, part=fp)\n",
    "        results.append(\n",
    "            TestDiff(metric=NMEMean, threshold=1)\n",
    "            .run(\n",
    "                model=model,\n",
    "                dataloader=dl,\n",
    "                dataloader_ref=dl_ref,\n",
    "                facial_part=fp,\n",
    "            )\n",
    "            .to_dict()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2A: Resized Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models.values():\n",
    "    dl = ResizedDataLoader(dl_ref, scales=0.5)\n",
    "    results.append(\n",
    "        TestDiff(metric=NMEMean, threshold=1)\n",
    "        .run(\n",
    "            model=model,\n",
    "            dataloader=dl,\n",
    "            dataloader_ref=dl_ref,\n",
    "        )\n",
    "        .to_dict()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2B: Recolored Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models.values():\n",
    "    dl = ColoredDataLoader(dl_ref)\n",
    "    results.append(\n",
    "        TestDiff(metric=NMEMean, threshold=1)\n",
    "        .run(\n",
    "            model=model,\n",
    "            dataloader=dl,\n",
    "            dataloader_ref=dl_ref,\n",
    "        )\n",
    "        .to_dict()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2C: Blurred Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models.values():\n",
    "    dl = BlurredDataLoader(dl_ref)\n",
    "    results.append(\n",
    "        TestDiff(metric=NMEMean, threshold=1)\n",
    "        .run(\n",
    "            model=model,\n",
    "            dataloader=dl,\n",
    "            dataloader_ref=dl_ref,\n",
    "        )\n",
    "        .to_dict()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3: Head Pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_dl = CachedDataLoader(HeadPoseDataLoader(dl_ref), cache_size=None, cache_img=False, cache_marks=False)\n",
    "\n",
    "\n",
    "def positive_roll(elt):\n",
    "    return elt[2][\"headPose\"][\"roll\"] > 0\n",
    "\n",
    "\n",
    "def negative_roll(elt):\n",
    "    return elt[2][\"headPose\"][\"roll\"] < 0\n",
    "\n",
    "\n",
    "head_poses = [positive_roll, negative_roll]\n",
    "\n",
    "for model in models.values():\n",
    "    for hp in head_poses:\n",
    "        dl = FilteredDataLoader(cached_dl, hp)\n",
    "        results.append(\n",
    "            TestDiff(metric=NMEMean, threshold=1)\n",
    "            .run(\n",
    "                model=model,\n",
    "                dataloader=dl,\n",
    "                dataloader_ref=dl_ref,\n",
    "            )\n",
    "            .to_dict()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 4: Ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 14:24:31.099843: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "ethnicity_dl = EthnicityDataLoader(dl_ref, ethnicity_map={\"indian\": \"asian\"})\n",
    "cached_dl = CachedDataLoader(ethnicity_dl, cache_size=None, cache_img=False, cache_marks=False)\n",
    "\n",
    "\n",
    "def white_ethnicity(elt):\n",
    "    return elt[2][\"ethnicity\"] == \"white\"\n",
    "\n",
    "\n",
    "ethnicities = [white_ethnicity]\n",
    "\n",
    "for model in models.values():\n",
    "    for e in ethnicities:\n",
    "        dl = FilteredDataLoader(cached_dl, e)\n",
    "        results.append(\n",
    "            TestDiff(metric=NMEMean, threshold=1)\n",
    "            .run(\n",
    "                model=model,\n",
    "                dataloader=dl,\n",
    "                dataloader_ref=dl_ref,\n",
    "            )\n",
    "            .to_dict()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>facial_part</th>\n",
       "      <th>dataloader</th>\n",
       "      <th>dataloader_ref</th>\n",
       "      <th>test</th>\n",
       "      <th>metric</th>\n",
       "      <th>metric_value</th>\n",
       "      <th>threshold</th>\n",
       "      <th>passed</th>\n",
       "      <th>prediction_time</th>\n",
       "      <th>prediction_fail_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OpenCV</td>\n",
       "      <td>left half</td>\n",
       "      <td>300W cropped on left half</td>\n",
       "      <td>300W</td>\n",
       "      <td>TestDiff</td>\n",
       "      <td>NME_mean</td>\n",
       "      <td>-0.644057</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.765928</td>\n",
       "      <td>0.564706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OpenCV</td>\n",
       "      <td>right half</td>\n",
       "      <td>300W cropped on right half</td>\n",
       "      <td>300W</td>\n",
       "      <td>TestDiff</td>\n",
       "      <td>NME_mean</td>\n",
       "      <td>-0.390821</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.632501</td>\n",
       "      <td>0.726471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OpenCV</td>\n",
       "      <td>entire face</td>\n",
       "      <td>300W resizing with ratios: 0.5</td>\n",
       "      <td>300W</td>\n",
       "      <td>TestDiff</td>\n",
       "      <td>NME_mean</td>\n",
       "      <td>-0.079876</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.619615</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OpenCV</td>\n",
       "      <td>entire face</td>\n",
       "      <td>300W altered with color mode 7</td>\n",
       "      <td>300W</td>\n",
       "      <td>TestDiff</td>\n",
       "      <td>NME_mean</td>\n",
       "      <td>0.001347</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.773121</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OpenCV</td>\n",
       "      <td>entire face</td>\n",
       "      <td>300W blurred</td>\n",
       "      <td>300W</td>\n",
       "      <td>TestDiff</td>\n",
       "      <td>NME_mean</td>\n",
       "      <td>-0.103017</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.776166</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OpenCV</td>\n",
       "      <td>entire face</td>\n",
       "      <td>(Cached (300W) with head-pose) filtered using 'positive_roll'</td>\n",
       "      <td>300W</td>\n",
       "      <td>TestDiff</td>\n",
       "      <td>NME_mean</td>\n",
       "      <td>0.077927</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.429424</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OpenCV</td>\n",
       "      <td>entire face</td>\n",
       "      <td>(Cached (300W) with head-pose) filtered using 'negative_roll'</td>\n",
       "      <td>300W</td>\n",
       "      <td>TestDiff</td>\n",
       "      <td>NME_mean</td>\n",
       "      <td>-0.019482</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.736994</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>OpenCV</td>\n",
       "      <td>entire face</td>\n",
       "      <td>(Cached (300W) with ethnicity) filtered using 'white_ethnicity'</td>\n",
       "      <td>300W</td>\n",
       "      <td>TestDiff</td>\n",
       "      <td>NME_mean</td>\n",
       "      <td>0.168421</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.675341</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model  facial_part  \\\n",
       "0  OpenCV    left half   \n",
       "1  OpenCV   right half   \n",
       "2  OpenCV  entire face   \n",
       "3  OpenCV  entire face   \n",
       "4  OpenCV  entire face   \n",
       "5  OpenCV  entire face   \n",
       "6  OpenCV  entire face   \n",
       "7  OpenCV  entire face   \n",
       "\n",
       "                                                        dataloader  \\\n",
       "0                                        300W cropped on left half   \n",
       "1                                       300W cropped on right half   \n",
       "2                                   300W resizing with ratios: 0.5   \n",
       "3                                   300W altered with color mode 7   \n",
       "4                                                     300W blurred   \n",
       "5    (Cached (300W) with head-pose) filtered using 'positive_roll'   \n",
       "6    (Cached (300W) with head-pose) filtered using 'negative_roll'   \n",
       "7  (Cached (300W) with ethnicity) filtered using 'white_ethnicity'   \n",
       "\n",
       "  dataloader_ref      test    metric  metric_value  threshold  passed  \\\n",
       "0           300W  TestDiff  NME_mean     -0.644057          1    True   \n",
       "1           300W  TestDiff  NME_mean     -0.390821          1    True   \n",
       "2           300W  TestDiff  NME_mean     -0.079876          1    True   \n",
       "3           300W  TestDiff  NME_mean      0.001347          1    True   \n",
       "4           300W  TestDiff  NME_mean     -0.103017          1    True   \n",
       "5           300W  TestDiff  NME_mean      0.077927          1    True   \n",
       "6           300W  TestDiff  NME_mean     -0.019482          1    True   \n",
       "7           300W  TestDiff  NME_mean      0.168421          1    True   \n",
       "\n",
       "   prediction_time  prediction_fail_rate  \n",
       "0         0.765928              0.564706  \n",
       "1         0.632501              0.726471  \n",
       "2         0.619615              0.000000  \n",
       "3         0.773121              0.000000  \n",
       "4         0.776166              0.000000  \n",
       "5         0.429424              0.000000  \n",
       "6         0.736994              0.000000  \n",
       "7         0.675341              0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# columns reordering\n",
    "report = pd.DataFrame(results)[\n",
    "    [\"model\", \"facial_part\", \"dataloader\", \"dataloader_ref\", \"test\", \"metric\", \"metric_value\", \"threshold\", \"passed\", \"prediction_time\", \"prediction_fail_rate\"]\n",
    "]\n",
    "report.groupby([\"model\"]).apply(display)  # display doesn't work in CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
