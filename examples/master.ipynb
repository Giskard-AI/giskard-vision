{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from face_alignment import FaceAlignment, LandmarksType\n",
    "\n",
    "from loreal_poc.dataloaders.loaders import DataLoaderFFHQ, DataLoader300W\n",
    "from loreal_poc.dataloaders.wrappers import (\n",
    "    CroppedDataLoader,\n",
    "    ResizedDataLoader,\n",
    "    ColoredDataLoader,\n",
    "    BlurredDataLoader,\n",
    "    FilteredDataLoader,\n",
    "    HeadPoseDataLoader,\n",
    "    EthnicityDataLoader,\n",
    "    CachedDataLoader,\n",
    ")\n",
    "\n",
    "from loreal_poc.models.wrappers import OpenCVWrapper, FaceAlignmentWrapper\n",
    "from loreal_poc.tests.performance import NMEMean\n",
    "from loreal_poc.tests.base import Test, TestDiff\n",
    "from loreal_poc.marks.facial_parts import FacialParts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading FFHQ dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl_ref = DataLoaderFFHQ(\"ffhq\")\n",
    "dl_ref = DataLoader300W(dir_path=\"300W/sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading landmark-detection models\n",
    "- FaceAlignment\n",
    "- OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from : lbfmodel.yaml\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"FaceAlignment\": FaceAlignmentWrapper(model=FaceAlignment(LandmarksType.TWO_D, device=\"cpu\", flip_input=False)),\n",
    "    \"OpenCV\": OpenCVWrapper(),\n",
    "}\n",
    "# models.pop(\"FaceAlignment\")  # takes a long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1: Cropped Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rak/Documents/loreal-poc/.venv/lib/python3.11/site-packages/face_alignment/api.py:147: UserWarning: No faces were detected.\n",
      "  warnings.warn(\"No faces were detected.\")\n",
      "FaceAlignmentWrapper: Face not detected in processed image of batch 3 and index 0.\n",
      "FaceAlignmentWrapper: Face not detected in processed image of batch 5 and index 0.\n",
      "/Users/rak/Documents/loreal-poc/loreal_poc/tests/performance.py:47: RuntimeWarning: Mean of empty slice\n",
      "  mes = np.nanmean(es, axis=1)\n",
      "OpenCVWrapper: Face not detected in processed image of batch 1 and index 0.\n",
      "OpenCVWrapper: Face not detected in processed image of batch 5 and index 0.\n",
      "OpenCVWrapper: Face not detected in processed image of batch 1 and index 0.\n",
      "OpenCVWrapper: Face not detected in processed image of batch 2 and index 0.\n",
      "OpenCVWrapper: Face not detected in processed image of batch 3 and index 0.\n",
      "OpenCVWrapper: Face not detected in processed image of batch 4 and index 0.\n"
     ]
    }
   ],
   "source": [
    "facial_parts = [FacialParts.LEFT_HALF.value, FacialParts.RIGHT_HALF.value]\n",
    "\n",
    "for model in models.values():\n",
    "    for fp in facial_parts:\n",
    "        dl = CroppedDataLoader(dl_ref, part=fp)\n",
    "        results.append(\n",
    "            TestDiff(metric=NMEMean, threshold=1)\n",
    "            .run(\n",
    "                model=model,\n",
    "                dataloader=dl,\n",
    "                dataloader_ref=dl_ref,\n",
    "                facial_part=fp,\n",
    "            )\n",
    "            .to_dict()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2A: Resized Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models.values():\n",
    "    dl = ResizedDataLoader(dl_ref, scales=0.5)\n",
    "    results.append(\n",
    "        TestDiff(metric=NMEMean, threshold=1)\n",
    "        .run(\n",
    "            model=model,\n",
    "            dataloader=dl,\n",
    "            dataloader_ref=dl_ref,\n",
    "        )\n",
    "        .to_dict()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2B: Recolored Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/rak/Documents/loreal-poc/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/jp/b7681vg128nf8s2hw47sl6380000gn/T/ipykernel_20755/771245311.py\", line 5, in <module>\n",
      "    .run(\n",
      "     ^^^^\n",
      "  File \"/Users/rak/Documents/loreal-poc/loreal_poc/tests/base.py\", line 146, in run\n",
      "    prediction_result = model.predict(dataloader, facial_part=facial_part)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/rak/Documents/loreal-poc/loreal_poc/models/base.py\", line 114, in predict\n",
      "    for images, _, _ in dataloader:\n",
      "  File \"/Users/rak/Documents/loreal-poc/loreal_poc/dataloaders/base.py\", line 91, in __next__\n",
      "    )\n",
      "      \n",
      "  File \"/Users/rak/Documents/loreal-poc/loreal_poc/dataloaders/base.py\", line 72, in __getitem__\n",
      "    def get_meta(self, idx: int) -> Optional[Dict]:\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/rak/Documents/loreal-poc/loreal_poc/dataloaders/base.py\", line 72, in <listcomp>\n",
      "    def get_meta(self, idx: int) -> Optional[Dict]:\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/rak/Documents/loreal-poc/loreal_poc/dataloaders/base.py\", line 68, in get_single_element\n",
      "  File \"/Users/rak/Documents/loreal-poc/loreal_poc/dataloaders/wrappers.py\", line 139, in get_image\n",
      "    return cv2.cvtColor(image, self._mode)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "cv2.error: OpenCV(4.8.1) /Users/runner/work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function 'cv::impl::(anonymous namespace)::CvtHelper<cv::impl::(anonymous namespace)::Set<3, 4, -1>, cv::impl::(anonymous namespace)::Set<1, -1, -1>, cv::impl::(anonymous namespace)::Set<0, 2, 5>, cv::impl::(anonymous namespace)::NONE>::CvtHelper(cv::InputArray, cv::OutputArray, int) [VScn = cv::impl::(anonymous namespace)::Set<3, 4, -1>, VDcn = cv::impl::(anonymous namespace)::Set<1, -1, -1>, VDepth = cv::impl::(anonymous namespace)::Set<0, 2, 5>, sizePolicy = cv::impl::(anonymous namespace)::NONE]'\n",
      "> Invalid number of channels in input image:\n",
      ">     'VScn::contains(scn)'\n",
      "> where\n",
      ">     'scn' is 1\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rak/Documents/loreal-poc/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/rak/Documents/loreal-poc/.venv/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/rak/Documents/loreal-poc/.venv/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/rak/Documents/loreal-poc/.venv/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/rak/Documents/loreal-poc/.venv/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/rak/Documents/loreal-poc/.venv/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/rak/Documents/loreal-poc/.venv/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Users/rak/Documents/loreal-poc/.venv/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/rak/Documents/loreal-poc/.venv/lib/python3.11/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/rak/Documents/loreal-poc/.venv/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/rak/Documents/loreal-poc/.venv/lib/python3.11/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/rak/Documents/loreal-poc/.venv/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/rak/Documents/loreal-poc/.venv/lib/python3.11/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"/Users/rak/Documents/loreal-poc/.venv/lib/python3.11/site-packages/executing/executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "for model in models.values():\n",
    "    dl = ColoredDataLoader(dl)\n",
    "    results.append(\n",
    "        TestDiff(metric=NMEMean, threshold=1)\n",
    "        .run(\n",
    "            model=model,\n",
    "            dataloader=dl,\n",
    "            dataloader_ref=dl_ref,\n",
    "        )\n",
    "        .to_dict()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2C: Blurred Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models.values():\n",
    "    dl = BlurredDataLoader(dl_ref)\n",
    "    results.append(\n",
    "        TestDiff(metric=NMEMean, threshold=1)\n",
    "        .run(\n",
    "            model=model,\n",
    "            dataloader=dl,\n",
    "            dataloader_ref=dl_ref,\n",
    "        )\n",
    "        .to_dict()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3: Head Pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_dl = CachedDataLoader(HeadPoseDataLoader(dl_ref), cache_size=None, cache_img=False, cache_marks=False)\n",
    "\n",
    "\n",
    "def positive_roll(elt):\n",
    "    return elt[2][\"headPose\"][\"roll\"] > 0\n",
    "\n",
    "\n",
    "def negative_roll(elt):\n",
    "    return elt[2][\"headPose\"][\"roll\"] < 0\n",
    "\n",
    "\n",
    "head_poses = [positive_roll, negative_roll]\n",
    "\n",
    "for model in models.values():\n",
    "    for hp in head_poses:\n",
    "        dl = FilteredDataLoader(cached_dl, hp)\n",
    "        results.append(\n",
    "            TestDiff(metric=NMEMean, threshold=1)\n",
    "            .run(\n",
    "                model=model,\n",
    "                dataloader=dl,\n",
    "                dataloader_ref=dl_ref,\n",
    "            )\n",
    "            .to_dict()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 4: Ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 12:11:36.469927: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "ethnicity_dl = EthnicityDataLoader(dl_ref, ethnicity_map={\"indian\": \"asian\"})\n",
    "cached_dl = CachedDataLoader(ethnicity_dl, cache_size=None, cache_img=False, cache_marks=False)\n",
    "\n",
    "\n",
    "def white_ethnicity(elt):\n",
    "    return elt[2][\"ethnicity\"] == \"white\"\n",
    "\n",
    "\n",
    "ethnicities = [white_ethnicity]\n",
    "\n",
    "for model in models.values():\n",
    "    for e in ethnicities:\n",
    "        dl = FilteredDataLoader(cached_dl, e)\n",
    "        results.append(\n",
    "            TestDiff(metric=NMEMean, threshold=1)\n",
    "            .run(\n",
    "                model=model,\n",
    "                dataloader=dl,\n",
    "                dataloader_ref=dl_ref,\n",
    "            )\n",
    "            .to_dict()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>facial_part</th>\n",
       "      <th>dataloader</th>\n",
       "      <th>dataloader_ref</th>\n",
       "      <th>test</th>\n",
       "      <th>metric</th>\n",
       "      <th>metric_value</th>\n",
       "      <th>threshold</th>\n",
       "      <th>passed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FaceAlignment</td>\n",
       "      <td>left half</td>\n",
       "      <td>300W cropped on left half</td>\n",
       "      <td>300W</td>\n",
       "      <td>TestDiff</td>\n",
       "      <td>NME_mean</td>\n",
       "      <td>-0.656253</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FaceAlignment</td>\n",
       "      <td>right half</td>\n",
       "      <td>300W cropped on right half</td>\n",
       "      <td>300W</td>\n",
       "      <td>TestDiff</td>\n",
       "      <td>NME_mean</td>\n",
       "      <td>-0.341428</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FaceAlignment</td>\n",
       "      <td>entire face</td>\n",
       "      <td>300W resizing with ratios: 0.5</td>\n",
       "      <td>300W</td>\n",
       "      <td>TestDiff</td>\n",
       "      <td>NME_mean</td>\n",
       "      <td>-0.011428</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FaceAlignment</td>\n",
       "      <td>entire face</td>\n",
       "      <td>300W resizing with ratios: 0.5 altered with color mode 7</td>\n",
       "      <td>300W</td>\n",
       "      <td>TestDiff</td>\n",
       "      <td>NME_mean</td>\n",
       "      <td>-0.047921</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FaceAlignment</td>\n",
       "      <td>entire face</td>\n",
       "      <td>300W blurred</td>\n",
       "      <td>300W</td>\n",
       "      <td>TestDiff</td>\n",
       "      <td>NME_mean</td>\n",
       "      <td>-0.009508</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FaceAlignment</td>\n",
       "      <td>entire face</td>\n",
       "      <td>(Cached (300W) with head-pose) filtered using 'positive_roll'</td>\n",
       "      <td>300W</td>\n",
       "      <td>TestDiff</td>\n",
       "      <td>NME_mean</td>\n",
       "      <td>0.146944</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>FaceAlignment</td>\n",
       "      <td>entire face</td>\n",
       "      <td>(Cached (300W) with head-pose) filtered using 'negative_roll'</td>\n",
       "      <td>300W</td>\n",
       "      <td>TestDiff</td>\n",
       "      <td>NME_mean</td>\n",
       "      <td>-0.036736</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FaceAlignment</td>\n",
       "      <td>entire face</td>\n",
       "      <td>(Cached (300W) with ethnicity) filtered using 'white_ethnicity'</td>\n",
       "      <td>300W</td>\n",
       "      <td>TestDiff</td>\n",
       "      <td>NME_mean</td>\n",
       "      <td>0.033779</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            model  facial_part  \\\n",
       "0   FaceAlignment    left half   \n",
       "1   FaceAlignment   right half   \n",
       "4   FaceAlignment  entire face   \n",
       "6   FaceAlignment  entire face   \n",
       "7   FaceAlignment  entire face   \n",
       "9   FaceAlignment  entire face   \n",
       "10  FaceAlignment  entire face   \n",
       "13  FaceAlignment  entire face   \n",
       "\n",
       "                                                         dataloader  \\\n",
       "0                                         300W cropped on left half   \n",
       "1                                        300W cropped on right half   \n",
       "4                                    300W resizing with ratios: 0.5   \n",
       "6          300W resizing with ratios: 0.5 altered with color mode 7   \n",
       "7                                                      300W blurred   \n",
       "9     (Cached (300W) with head-pose) filtered using 'positive_roll'   \n",
       "10    (Cached (300W) with head-pose) filtered using 'negative_roll'   \n",
       "13  (Cached (300W) with ethnicity) filtered using 'white_ethnicity'   \n",
       "\n",
       "   dataloader_ref      test    metric  metric_value  threshold  passed  \n",
       "0            300W  TestDiff  NME_mean     -0.656253          1    True  \n",
       "1            300W  TestDiff  NME_mean     -0.341428          1    True  \n",
       "4            300W  TestDiff  NME_mean     -0.011428          1    True  \n",
       "6            300W  TestDiff  NME_mean     -0.047921          1    True  \n",
       "7            300W  TestDiff  NME_mean     -0.009508          1    True  \n",
       "9            300W  TestDiff  NME_mean      0.146944          1    True  \n",
       "10           300W  TestDiff  NME_mean     -0.036736          1    True  \n",
       "13           300W  TestDiff  NME_mean      0.033779          1    True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>facial_part</th>\n",
       "      <th>dataloader</th>\n",
       "      <th>dataloader_ref</th>\n",
       "      <th>test</th>\n",
       "      <th>metric</th>\n",
       "      <th>metric_value</th>\n",
       "      <th>threshold</th>\n",
       "      <th>passed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OpenCV</td>\n",
       "      <td>left half</td>\n",
       "      <td>300W cropped on left half</td>\n",
       "      <td>300W</td>\n",
       "      <td>TestDiff</td>\n",
       "      <td>NME_mean</td>\n",
       "      <td>-0.644057</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OpenCV</td>\n",
       "      <td>right half</td>\n",
       "      <td>300W cropped on right half</td>\n",
       "      <td>300W</td>\n",
       "      <td>TestDiff</td>\n",
       "      <td>NME_mean</td>\n",
       "      <td>-0.390821</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OpenCV</td>\n",
       "      <td>entire face</td>\n",
       "      <td>300W resizing with ratios: 0.5</td>\n",
       "      <td>300W</td>\n",
       "      <td>TestDiff</td>\n",
       "      <td>NME_mean</td>\n",
       "      <td>-0.079876</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>OpenCV</td>\n",
       "      <td>entire face</td>\n",
       "      <td>300W blurred</td>\n",
       "      <td>300W</td>\n",
       "      <td>TestDiff</td>\n",
       "      <td>NME_mean</td>\n",
       "      <td>-0.103017</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>OpenCV</td>\n",
       "      <td>entire face</td>\n",
       "      <td>(Cached (300W) with head-pose) filtered using 'positive_roll'</td>\n",
       "      <td>300W</td>\n",
       "      <td>TestDiff</td>\n",
       "      <td>NME_mean</td>\n",
       "      <td>0.077927</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>OpenCV</td>\n",
       "      <td>entire face</td>\n",
       "      <td>(Cached (300W) with head-pose) filtered using 'negative_roll'</td>\n",
       "      <td>300W</td>\n",
       "      <td>TestDiff</td>\n",
       "      <td>NME_mean</td>\n",
       "      <td>-0.019482</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>OpenCV</td>\n",
       "      <td>entire face</td>\n",
       "      <td>(Cached (300W) with ethnicity) filtered using 'white_ethnicity'</td>\n",
       "      <td>300W</td>\n",
       "      <td>TestDiff</td>\n",
       "      <td>NME_mean</td>\n",
       "      <td>0.168421</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     model  facial_part  \\\n",
       "2   OpenCV    left half   \n",
       "3   OpenCV   right half   \n",
       "5   OpenCV  entire face   \n",
       "8   OpenCV  entire face   \n",
       "11  OpenCV  entire face   \n",
       "12  OpenCV  entire face   \n",
       "14  OpenCV  entire face   \n",
       "\n",
       "                                                         dataloader  \\\n",
       "2                                         300W cropped on left half   \n",
       "3                                        300W cropped on right half   \n",
       "5                                    300W resizing with ratios: 0.5   \n",
       "8                                                      300W blurred   \n",
       "11    (Cached (300W) with head-pose) filtered using 'positive_roll'   \n",
       "12    (Cached (300W) with head-pose) filtered using 'negative_roll'   \n",
       "14  (Cached (300W) with ethnicity) filtered using 'white_ethnicity'   \n",
       "\n",
       "   dataloader_ref      test    metric  metric_value  threshold  passed  \n",
       "2            300W  TestDiff  NME_mean     -0.644057          1    True  \n",
       "3            300W  TestDiff  NME_mean     -0.390821          1    True  \n",
       "5            300W  TestDiff  NME_mean     -0.079876          1    True  \n",
       "8            300W  TestDiff  NME_mean     -0.103017          1    True  \n",
       "11           300W  TestDiff  NME_mean      0.077927          1    True  \n",
       "12           300W  TestDiff  NME_mean     -0.019482          1    True  \n",
       "14           300W  TestDiff  NME_mean      0.168421          1    True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# columns reordering\n",
    "report = pd.DataFrame(results)[\n",
    "    [\"model\", \"facial_part\", \"dataloader\", \"dataloader_ref\", \"test\", \"metric\", \"metric_value\", \"threshold\", \"passed\"]\n",
    "]\n",
    "report.groupby([\"model\"]).apply(display)  # display doesn't work in CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
